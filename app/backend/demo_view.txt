$ python app/backend/rag.py
USER_AGENT environment variable not set, consider setting it to identify your requests.
API Key found
URL Key found
Connected to Qdrant
HuggingFace embeddings model loaded successfully.
D:\CODING\Remix\RAG_APP\app\backend\qdrant.py:28: LangChainDeprecationWarning: The class `Qdrant` was deprecated in LangChain 0.1.2 and will be removed in 0.5.0. Use :class:`~QdrantVectorStore` instead.
  vector_store = Qdrant(client=client, collection_name=collection_name, embeddings=embeddings_model)
Model loaded successfully: client=<groq.resources.chat.completions.Completions object at 0x000002298019AC00> async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000002298019B6E0> temperature=1.4 model_kwargs={} groq_api_key=SecretStr('**********')✔
Chain created
{'answer': '- The blog is about Generative AI, its applications, and how it works.\n- It explains the evolution of generative AI, the rise of deep generative models, and the role of Transformers in this field.\n- Generative AI models are deep-learning models that can take raw data and generate statistically probable outputs when prompted.\n- Transformers, introduced by Google, significantly changed how language models were trained by combining the encoder-decoder architecture with a text-processing mechanism called attention.\n- The author of the blog is Kate Soule, an expert from IBM Research.', 
'context': [Document(metadata={'source': 'https://research.ibm.com/blog/what-is-generative-AI', '_id': '529a663e-cc37-4022-9715-e6de3263313f', '_collection_name': 'websites'}, page_content='gone through many cycles of hype, but even to skeptics, the release of ChatGPT seems to mark a turning point. OpenAI’s chatbot, powered by its latest large language model, can write poems, tell jokes, and churn out essays that look like a human created them. Prompt ChatGPT with a few words, and out comes love poems in the form of Yelp reviews, or song lyrics in the style of Nick Cave.The last time generative AI loomed this large, the breakthroughs were in computer vision. Selfies transformed into Renaissance-style portraits and prematurely aged faces filled social media feeds. Five years later, it’s the leap forward in natural language processing, and the ability of large language models to riff on just about any theme, that has seized the popular imagination. And it’s not just language: Generative models can also learn the grammar of software code, molecules, natural images, and a variety of other data types.The applications for this technology are growing every day, and we’re just'), Document(metadata={'source': 'https://research.ibm.com/blog/what-is-generative-AI', '_id': 'cb1e60a6-b39a-49e1-b8b6-91c19f5ae60a', '_collection_name': 'websites'}, page_content='AINatural Language ProcessingIBM announces the winners of the 2024 Pat Goldberg competitionNews05 Nov 2024AIQuantumSemiconductorsBuilding the future of chips in the USANewsMike Murphy and Peter Hess05 Nov 2024AIQuantumSemiconductorsIBM LogoFocus areasFocus areasSemiconductorsArtificial IntelligenceQuantum ComputingHybrid CloudQuick linksQuick linksAboutPublicationsBlogEventsWork with usWork with usCareersCollaborateContact ResearchDirectoriesDirectoriesTopicsPeopleProjectsFollow usFollow usNewsletterXLinkedInYouTubeContact IBMPrivacyTerms of useAccessibility'), Document(metadata={'source': 'https://research.ibm.com/blog/what-is-generative-AI', '_id': '118741fd-279a-4aa6-a16b-9e16b007b983', '_collection_name': 'websites'}, page_content='day, and we’re just starting to explore the possibilities. At IBM Research, we’re working to help our customers use generative models to write high-quality software code faster, discover new molecules, and train trustworthy conversational chatbots grounded on enterprise data. We’re even using generative AI to create synthetic data to build more robust and trustworthy AI models and to stand-in for real data protected by privacy and copyright laws.As the field continues to evolve, we thought we’d take a step back and explain what we mean by generative AI, how we got here, and how these models work.An overview from IBM expert Kate SouleWhat are generative AI models?The rise of deep generative modelsGenerative AI refers to deep-learning models that can take raw data — say, all of Wikipedia or the collected works of Rembrandt — and “learn” to generate statistically probable outputs when prompted. At a high level, generative models encode a simplified representation of their training data'), Document(metadata={'source': 'https://research.ibm.com/blog/what-is-generative-AI', '_id': 'fe1843af-abc4-463b-a22c-bba055085659', '_collection_name': 'websites'}, page_content='most important features.Transformers, introduced by Google in 2017 in a landmark paper  “Attention Is All You Need,” combined the encoder-decoder architecture with a text-processing mechanism called attention to change how language models were trained. An encoder converts raw unannotated text into representations known as embeddings; the decoder takes these embeddings together with previous outputs of the model, and successively predicts each word in a sentence.Through fill-in-the-blank guessing games, the encoder learns how words and sentences relate to each other, building up a powerful representation of language without anyone having to label parts of speech and other grammatical features. Transformers, in fact, can be pre-trained at the outset without a particular task in mind. Once these powerful representations are learned, the models can later be specialized — with much less data — to perform a given task.Several innovations made this possible. Transformers processed words in a')]}